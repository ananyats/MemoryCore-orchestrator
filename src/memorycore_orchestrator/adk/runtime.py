"""Lightweight Agent Development Kit (ADK) runtime primitives.

The real OpenAI ADK offers an extensive runtime that manages tools, memory,
state tracking, and orchestration between agents.  For the purpose of this
repository we create a deliberately small subset of the API surface so that
teams can experiment with orchestration concepts without needing the hosted
service.

The :class:`AgentRuntime` owns a single large language model (LLM) client and
provides helpers for creating prompts and capturing model responses.  Agents
built on top of this runtime only need to supply a prompt template and the
parameters that should be injected into it.
"""
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, Protocol


class LLMClient(Protocol):
    """Protocol implemented by language model clients.

    The protocol is intentionally tiny so that tests can supply a stub client
    without requiring network access.
    """

    def complete(self, prompt: str, *, temperature: float = 0.0) -> str:
        """Return the model completion for ``prompt``."""


class OpenAIClient:
    """Thin wrapper around the official OpenAI SDK.

    The wrapper is optional because we do not make any network calls in the
    automated tests.  Importing the dependency lazily keeps the repository
    usable even when the SDK is not installed.  Users that wish to exercise the
    runtime with a live model need to install the ``openai`` package and supply
    the ``OPENAI_API_KEY`` environment variable.
    """

    def __init__(self, *, model: str = "gpt-4o-mini", temperature: float = 0.0):
        try:
            from openai import OpenAI  # type: ignore
        except ImportError as exc:  # pragma: no cover - optional dependency
            raise RuntimeError(
                "The openai package is required to use OpenAIClient. Install it via"
                " `pip install openai` and try again."
            ) from exc

        self._client = OpenAI()
        self._model = model
        self._temperature = temperature

    def complete(self, prompt: str, *, temperature: float | None = None) -> str:
        temperature = self._temperature if temperature is None else temperature
        response = self._client.responses.create(  # pragma: no cover - network call
            model=self._model,
            input=prompt,
            temperature=temperature,
        )
        return response.output_text  # type: ignore[attr-defined]


@dataclass
class AgentRuntime:
    """Configuration shared across all agents.

    Parameters
    ----------
    llm_client:
        Implementation of :class:`LLMClient` used to generate completions.
    default_temperature:
        Temperature applied to the LLM calls when an agent does not specify one.
    context:
        Mutable dictionary that agents can use to exchange information.
    """

    llm_client: LLMClient
    default_temperature: float = 0.0
    context: Dict[str, str] = field(default_factory=dict)

    def complete(self, prompt: str, *, temperature: float | None = None) -> str:
        """Return a completion generated by the runtime's LLM client."""

        used_temperature = self.default_temperature if temperature is None else temperature
        return self.llm_client.complete(prompt, temperature=used_temperature)


__all__ = ["AgentRuntime", "LLMClient", "OpenAIClient"]
